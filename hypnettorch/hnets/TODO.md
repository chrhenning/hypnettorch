# TODOs

* Implement a recurrent hypernetwork, that can produce a different set of weights per timestep for an arbitrary number of timesteps. Therefore, we have to be able to leave target shapes more flexible, since the number of weights to be produced will be determined at run time.
* Implement a recurrent hypernetwork, that can takeover the job of the chunked MLP hypernetwork [ChunkedHMLP](hnets/chunked_mlp_hnet.py) without the need of having chunk embeddings that increase the parameter count. It could also be used to produce a flexible number of outputs (for multiple architectures) or weights per timestep of a recurrent main network (such as the [SimpleRNN](mnets/simple_rnn.py)). Therefore, we should design dedicated recurrent layers (similar to [BatchNormLayer](utils/batchnorm_layer.py)), that can operate in both modes; compute only one timestep at a time or compute all timesteps at once.
