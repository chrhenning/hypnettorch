{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured Hyper-Chunking of a ResNet-32\n",
    "\n",
    "This tutorial shows how to utilize a [StructuredHMLP](https://hypnettorch.readthedocs.io/en/latest/hnets.html#structured-chunked-mlp-hypernetwork) (a certain kind of hypernetwork that allows *smart* chunking) in combination with a [ResNet](https://hypnettorch.readthedocs.io/en/latest/mnets.html#resnet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure code of repository is visible to this tutorial.\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from hypnettorch.hnets.structured_hmlp_examples import resnet_chunking\n",
    "from hypnettorch.hnets import StructuredHMLP\n",
    "from hypnettorch.mnets import ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a Resnet-32\n",
    "\n",
    "First, we instantiate a Resnet-32 (a resnet contains $6n+2$ layers $\\rightarrow$ 32 layers for $n=5$) with no internal weights (`no_weights=True`). Thus, it's weights are expected to originate externally (in our case from a hypernetwork) and to be passed to its `forward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ResNet with 32 layers and 462760 weights is created. The network uses batchnorm.\n"
     ]
    }
   ],
   "source": [
    "net = ResNet(use_bias=False, use_batch_norm=True, no_weights=True, n=5, num_feature_maps=[8, 16, 32, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide how to *chunk* the resnet\n",
    "\n",
    "Next, we need to decide how we can chunk the weights of the main network `net` in a smart way. We therefore utilize a helper function provided in module [structured_hmlp_examples](https://hypnettorch.readthedocs.io/en/latest/hnets.html#example-instantiations-of-a-structured-chunked-mlp-hypernetwork). This helper function treats all weights in the first and last layer as one chunk each. The $6*n = 30$ hidden layers are chunked in a smart way. In our case (using `gcd_chunking=True`), the function will first compute the greatest common divisor (gcd) of the channel sizes `num_feature_maps=[8, 16, 32, 64]`. The output of each hidden layer can then be split into chunks of this size (gcd). This way, only 4 chunks are needed to build all weight tensors of the hidden layers (note, that the hidden convolutional layers can have 4 different channel input sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hypernetwork is expected to produce the following chunks which are then assembled to the resnet its internal weights:\n",
      "* 1 chunks of shape [[8, 3, 3, 3], [8], [8]]\n",
      "* 2 chunks of shape [[8, 8, 3, 3], [8], [8]]\n",
      "* 22 chunks of shape [[8, 16, 3, 3], [8], [8]]\n",
      "* 44 chunks of shape [[8, 32, 3, 3], [8], [8]]\n",
      "* 72 chunks of shape [[8, 64, 3, 3], [8], [8]]\n",
      "* 1 chunks of shape [[10, 64]]\n"
     ]
    }
   ],
   "source": [
    "chunk_shapes, num_per_chunk, assembly_fct = resnet_chunking(net, gcd_chunking=True)\n",
    "\n",
    "# Print specified way of chunking.\n",
    "print('The hypernetwork is expected to produce the following chunks which are then assembled to the resnet its internal weights:')\n",
    "for i, s in enumerate(chunk_shapes):\n",
    "    print('* %d chunks of shape %s' % (num_per_chunk[i], s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate hypernetwork\n",
    "\n",
    "Now, we can instantiate the hypernetwork; providing all the information we collected above. Note, the function handle `assembly_fct` will tell the hypernetwork how to reassemble the chunking above into the target shapes expected by the resnet (`net.hyper_shapes_learned`).\n",
    "\n",
    "We instantiate the hypernetwork, such that it doesn't expect any external input. Instead, the chunk embeddings are the only input to the internal hypernetworks which create the above specified chunks. They are conditional, such that upon receiving a conditional ID a set of chunk embeddings is selected by the hypernetwork internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Structured Chunked MLP Hypernet.\n",
      "It manages 6 full hypernetworks internally that produce 142 chunks in total.\n",
      "The internal hypernetworks have a combined output size of 9576 compared to 462760 weights produced by this network.\n",
      "Hypernetwork with 117896 weights and 462760 outputs (compression ratio: 0.25).\n",
      "The network consists of 106536 unconditional weights (106536 internally maintained) and 11360 conditional weights (11360 internally maintained).\n"
     ]
    }
   ],
   "source": [
    "chunk_emb_size = 8\n",
    "hnet = StructuredHMLP(net.hyper_shapes_learned, chunk_shapes, num_per_chunk, \n",
    "                      chunk_emb_size, {'layers': [10,10]}, assembly_fct,\n",
    "                      cond_chunk_embs=True, uncond_in_size=0, cond_in_size=0,\n",
    "                      verbose=True, no_uncond_weights=False, no_cond_weights=False,\n",
    "                      num_cond_embs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate weights and use them in the main network\n",
    "\n",
    "Lastly, we show how to generate two sets of weights and how to use those weights to make predictions with the main network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions made with weights of condition \"1\": [0.  0.  0.  0.  0.7 0.  0.  0.  0.1 0.2]\n",
      "Predictions made with weights of condition \"3\": [0.1 0.2 0.  0.  0.1 0.  0.  0.  0.1 0.5]\n"
     ]
    }
   ],
   "source": [
    "cond_ids = [1, 3]\n",
    "weights = hnet.forward(cond_id=cond_ids)\n",
    "\n",
    "# Generate batch of random images.\n",
    "# Note, due to the data handlers currently used in this repository,\n",
    "# the resnet expects a batch of flattened images as input.\n",
    "x = torch.rand(9, 32 * 32 * 3)\n",
    "y1 = net.forward(x, weights=weights[0])\n",
    "y2 = net.forward(x, weights=weights[1])\n",
    "\n",
    "y1 = torch.nn.functional.softmax(y1, dim=1)\n",
    "y2 = torch.nn.functional.softmax(y2, dim=1)\n",
    "\n",
    "for cid, pred in zip(cond_ids, [y1, y2]):\n",
    "    print('Predictions made with weights of condition \"%d\": %s' \\\n",
    "          % (cid, np.array2string(pred[0,:].detach().numpy(), precision=1, suppress_small=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
